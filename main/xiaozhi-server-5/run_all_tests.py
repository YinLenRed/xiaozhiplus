#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°æ™ºç³»ç»Ÿå…¨å¥—æµ‹è¯•ä¸€é”®è¿è¡Œè„šæœ¬
è‡ªåŠ¨åŒ–è¿è¡Œæ‰€æœ‰æµ‹è¯•ç»„ä»¶å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Š
"""

import asyncio
import json
import logging
import time
import sys
import os
import subprocess
from datetime import datetime
from typing import Dict, List, Optional
import argparse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'test_logs/master_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger('MasterTest')

class TestSuiteConfig:
    """æµ‹è¯•å¥—ä»¶é…ç½®"""
    
    def __init__(self):
        # æœåŠ¡é…ç½®
        self.java_api_url = "http://q83b6ed9.natappfree.cc"
        self.python_api_url = "http://47.98.51.180:8003"
        self.mqtt_host = "47.97.185.142"
        self.mqtt_port = 1883
        self.websocket_url = "ws://47.98.51.180:8000/xiaozhi/v1/"
        
        # è®¾å¤‡é…ç½®
        self.test_device_id = "f0:9e:9e:04:8a:44"
        
        # æµ‹è¯•é…ç½®
        self.enable_hardware_simulator = True
        self.enable_stress_tests = True
        self.concurrent_tests = 3
        self.test_timeout = 300  # 5åˆ†é’Ÿè¶…æ—¶
        
        # æ–‡ä»¶è·¯å¾„
        self.test_scripts = {
            'java_api': 'test_java_api.py',
            'mqtt': 'test_mqtt_communication.py',
            'websocket': 'test_websocket_audio.py',
            'full_flow': 'test_full_flow.py'
        }
        self.simulator_script = 'hardware_simulator.py'
        
        # æŠ¥å‘Šé…ç½®
        self.output_dir = 'test_reports'
        self.individual_reports = True
        self.generate_html_report = True

class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config: TestSuiteConfig):
        self.config = config
        self.test_results = {}
        self.simulator_process = None
        self.start_time = None
        self.end_time = None
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.config.output_dir, exist_ok=True)
        os.makedirs('test_logs', exist_ok=True)
    
    async def setup_environment(self) -> bool:
        """ç¯å¢ƒå‡†å¤‡"""
        logger.info("ğŸ”§ å¼€å§‹ç¯å¢ƒå‡†å¤‡...")
        
        try:
            # æ£€æŸ¥å¿…è¦çš„è„šæœ¬æ–‡ä»¶
            missing_scripts = []
            for test_name, script_path in self.config.test_scripts.items():
                if not os.path.exists(script_path):
                    missing_scripts.append(script_path)
            
            if missing_scripts:
                logger.error(f"âŒ ç¼ºå°‘æµ‹è¯•è„šæœ¬: {', '.join(missing_scripts)}")
                return False
            
            # å¯åŠ¨ç¡¬ä»¶æ¨¡æ‹Ÿå™¨
            if self.config.enable_hardware_simulator:
                await self._start_hardware_simulator()
            
            # ç­‰å¾…æœåŠ¡å‡†å¤‡å°±ç»ª
            await self._wait_for_services()
            
            logger.info("âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç¯å¢ƒå‡†å¤‡å¤±è´¥: {e}")
            return False
    
    async def _start_hardware_simulator(self):
        """å¯åŠ¨ç¡¬ä»¶æ¨¡æ‹Ÿå™¨"""
        try:
            if not os.path.exists(self.config.simulator_script):
                logger.warning(f"âš ï¸  ç¡¬ä»¶æ¨¡æ‹Ÿå™¨è„šæœ¬ä¸å­˜åœ¨: {self.config.simulator_script}")
                return
            
            logger.info("ğŸ¤– å¯åŠ¨ç¡¬ä»¶è®¾å¤‡æ¨¡æ‹Ÿå™¨...")
            
            # å¯åŠ¨æ¨¡æ‹Ÿå™¨è¿›ç¨‹
            self.simulator_process = subprocess.Popen([
                sys.executable, self.config.simulator_script,
                self.config.test_device_id
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            
            # ç­‰å¾…æ¨¡æ‹Ÿå™¨å¯åŠ¨
            await asyncio.sleep(5)
            
            # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
            if self.simulator_process.poll() is None:
                logger.info("âœ… ç¡¬ä»¶æ¨¡æ‹Ÿå™¨å¯åŠ¨æˆåŠŸ")
            else:
                logger.error("âŒ ç¡¬ä»¶æ¨¡æ‹Ÿå™¨å¯åŠ¨å¤±è´¥")
                self.simulator_process = None
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨ç¡¬ä»¶æ¨¡æ‹Ÿå™¨å¼‚å¸¸: {e}")
    
    async def _wait_for_services(self):
        """ç­‰å¾…æœåŠ¡å‡†å¤‡å°±ç»ª"""
        import requests
        
        services = [
            ("Java API", self.config.java_api_url + "/actuator/health"),
            ("Python API", self.config.python_api_url + "/check/hello")
        ]
        
        for service_name, health_url in services:
            logger.info(f"â³ ç­‰å¾… {service_name} å‡†å¤‡å°±ç»ª...")
            
            for attempt in range(30):  # æœ€å¤šç­‰å¾…30ç§’
                try:
                    response = requests.get(health_url, timeout=2)
                    if response.status_code == 200:
                        logger.info(f"âœ… {service_name} å·²å°±ç»ª")
                        break
                except:
                    pass
                
                await asyncio.sleep(1)
            else:
                logger.warning(f"âš ï¸  {service_name} å¥åº·æ£€æŸ¥è¶…æ—¶")
    
    async def run_single_test(self, test_name: str, script_path: str, args: List[str] = None) -> Dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        logger.info(f"ğŸ§ª å¼€å§‹è¿è¡Œæµ‹è¯•: {test_name}")
        
        start_time = time.time()
        
        try:
            # æ„å»ºå‘½ä»¤è¡Œå‚æ•°
            cmd = [sys.executable, script_path]
            if args:
                cmd.extend(args)
            
            # æ·»åŠ é€šç”¨å‚æ•°
            cmd.extend([
                '--device-id', self.config.test_device_id,
                '--report', os.path.join(self.config.output_dir, f'{test_name}_report.json')
            ])
            
            # æ ¹æ®æµ‹è¯•ç±»å‹æ·»åŠ ç‰¹å®šå‚æ•°
            if test_name == 'java_api':
                cmd.extend([
                    '--java-url', self.config.java_api_url,
                    '--python-url', self.config.python_api_url
                ])
            elif test_name == 'mqtt':
                cmd.extend([
                    '--host', self.config.mqtt_host,
                    '--port', str(self.config.mqtt_port),
                    '--concurrent', str(self.config.concurrent_tests)
                ])
            elif test_name == 'websocket':
                cmd.extend([
                    '--websocket-url', self.config.websocket_url,
                    '--concurrent', str(self.config.concurrent_tests)
                ])
            
            logger.info(f"ğŸ“¤ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # è¿è¡Œæµ‹è¯•è„šæœ¬
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            # ç­‰å¾…æµ‹è¯•å®Œæˆï¼Œå¸¦è¶…æ—¶
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=self.config.test_timeout
                )
            except asyncio.TimeoutError:
                process.kill()
                await process.wait()
                raise Exception(f"æµ‹è¯•è¶…æ—¶ï¼ˆ{self.config.test_timeout}ç§’ï¼‰")
            
            end_time = time.time()
            duration = end_time - start_time
            
            # åˆ†ææµ‹è¯•ç»“æœ
            success = process.returncode == 0
            stdout_text = stdout.decode('utf-8') if stdout else ""
            stderr_text = stderr.decode('utf-8') if stderr else ""
            
            result = {
                'test_name': test_name,
                'success': success,
                'duration': duration,
                'return_code': process.returncode,
                'stdout': stdout_text,
                'stderr': stderr_text,
                'command': ' '.join(cmd),
                'start_time': start_time,
                'end_time': end_time
            }
            
            # å°è¯•è¯»å–è¯¦ç»†æŠ¥å‘Š
            report_file = os.path.join(self.config.output_dir, f'{test_name}_report.json')
            if os.path.exists(report_file):
                try:
                    with open(report_file, 'r', encoding='utf-8') as f:
                        detailed_report = json.load(f)
                        result['detailed_report'] = detailed_report
                except Exception as e:
                    logger.warning(f"âš ï¸  è¯»å–è¯¦ç»†æŠ¥å‘Šå¤±è´¥ {test_name}: {e}")
            
            if success:
                logger.info(f"âœ… æµ‹è¯•å®Œæˆ: {test_name} (è€—æ—¶: {duration:.1f}ç§’)")
            else:
                logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {test_name} (è€—æ—¶: {duration:.1f}ç§’)")
                if stderr_text:
                    logger.error(f"   é”™è¯¯ä¿¡æ¯: {stderr_text[:500]}...")
            
            return result
            
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            
            logger.error(f"âŒ æµ‹è¯•å¼‚å¸¸: {test_name} - {e}")
            
            return {
                'test_name': test_name,
                'success': False,
                'duration': duration,
                'error': str(e),
                'start_time': start_time,
                'end_time': end_time
            }
    
    async def run_all_tests(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶")
        self.start_time = time.time()
        
        try:
            # ç¯å¢ƒå‡†å¤‡
            if not await self.setup_environment():
                return {"success": False, "error": "ç¯å¢ƒå‡†å¤‡å¤±è´¥"}
            
            # æŒ‰é¡ºåºè¿è¡Œæµ‹è¯•
            test_order = ['java_api', 'mqtt', 'websocket', 'full_flow']
            
            for test_name in test_order:
                if test_name in self.config.test_scripts:
                    script_path = self.config.test_scripts[test_name]
                    result = await self.run_single_test(test_name, script_path)
                    self.test_results[test_name] = result
                    
                    # æµ‹è¯•å¤±è´¥åæ˜¯å¦ç»§ç»­
                    if not result['success']:
                        logger.warning(f"âš ï¸  {test_name} æµ‹è¯•å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œåç»­æµ‹è¯•")
                        
                    # æµ‹è¯•é—´éš”
                    await asyncio.sleep(2)
            
            self.end_time = time.time()
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = await self.generate_comprehensive_report()
            
            return comprehensive_report
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•å¥—ä»¶æ‰§è¡Œå¼‚å¸¸: {e}")
            return {"success": False, "error": str(e)}
        
        finally:
            await self.cleanup()
    
    async def generate_comprehensive_report(self) -> Dict:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š...")
        
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results.values() if r['success'])
        failed_tests = total_tests - successful_tests
        
        total_duration = self.end_time - self.start_time if self.start_time and self.end_time else 0
        
        # æ”¶é›†å„æµ‹è¯•çš„è¯¦ç»†ç»Ÿè®¡
        test_statistics = {}
        for test_name, result in self.test_results.items():
            detailed = result.get('detailed_report', {})
            test_summary = detailed.get('test_summary', {})
            
            test_statistics[test_name] = {
                'success': result['success'],
                'duration': result['duration'],
                'sub_tests': test_summary.get('total_tests', 0),
                'sub_successful': test_summary.get('successful_tests', 0),
                'sub_success_rate': test_summary.get('success_rate', 0),
                'error': result.get('error'),
                'return_code': result.get('return_code')
            }
        
        # æ„å»ºç»¼åˆæŠ¥å‘Š
        comprehensive_report = {
            'master_summary': {
                'test_suite_version': '1.0.0',
                'total_test_categories': total_tests,
                'successful_categories': successful_tests,
                'failed_categories': failed_tests,
                'overall_success_rate': successful_tests / total_tests * 100 if total_tests > 0 else 0,
                'total_execution_time': total_duration,
                'start_time': datetime.fromtimestamp(self.start_time).isoformat() if self.start_time else None,
                'end_time': datetime.fromtimestamp(self.end_time).isoformat() if self.end_time else None
            },
            'test_configuration': {
                'java_api_url': self.config.java_api_url,
                'python_api_url': self.config.python_api_url,
                'mqtt_server': f"{self.config.mqtt_host}:{self.config.mqtt_port}",
                'websocket_url': self.config.websocket_url,
                'test_device_id': self.config.test_device_id,
                'hardware_simulator_used': self.simulator_process is not None,
                'concurrent_tests': self.config.concurrent_tests,
                'test_timeout': self.config.test_timeout
            },
            'test_category_results': test_statistics,
            'detailed_test_results': self.test_results,
            'system_recommendations': self._generate_recommendations()
        }
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        master_report_file = os.path.join(
            self.config.output_dir,
            f'master_test_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        )
        
        with open(master_report_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {master_report_file}")
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        if self.config.generate_html_report:
            html_report_file = master_report_file.replace('.json', '.html')
            self._generate_html_report(comprehensive_report, html_report_file)
        
        return comprehensive_report
    
    def _generate_recommendations(self) -> List[str]:
        """åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        for test_name, result in self.test_results.items():
            if not result['success']:
                if test_name == 'java_api':
                    recommendations.append("æ£€æŸ¥Java APIæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œç«¯å£æ˜¯å¦æ­£ç¡®")
                elif test_name == 'mqtt':
                    recommendations.append("æ£€æŸ¥MQTTæœåŠ¡å™¨è¿æ¥ï¼Œç¡®è®¤ç”¨æˆ·åå¯†ç æ­£ç¡®")
                elif test_name == 'websocket':
                    recommendations.append("æ£€æŸ¥WebSocketæœåŠ¡å™¨çŠ¶æ€ï¼Œç¡®è®¤éŸ³é¢‘ä¼ è¾“åŠŸèƒ½")
                elif test_name == 'full_flow':
                    recommendations.append("æ£€æŸ¥ç«¯åˆ°ç«¯æµç¨‹ï¼Œå¯èƒ½éœ€è¦ç¡¬ä»¶è®¾å¤‡æˆ–æ¨¡æ‹Ÿå™¨")
            
            detailed = result.get('detailed_report', {})
            if detailed:
                # åˆ†æè¯¦ç»†æŠ¥å‘Šä¸­çš„é—®é¢˜
                test_summary = detailed.get('test_summary', {})
                success_rate = test_summary.get('success_rate', 100)
                
                if success_rate < 100:
                    recommendations.append(f"{test_name}æµ‹è¯•æˆåŠŸç‡{success_rate:.1f}%ï¼Œå»ºè®®æ£€æŸ¥ç›¸å…³é…ç½®")
        
        if not recommendations:
            recommendations.append("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼")
        
        return recommendations
    
    def _generate_html_report(self, report_data: Dict, html_file: str):
        """ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š"""
        try:
            html_content = self._create_html_content(report_data)
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.info(f"ğŸ“„ HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")
        except Exception as e:
            logger.error(f"âŒ HTMLæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    
    def _create_html_content(self, report_data: Dict) -> str:
        """åˆ›å»ºHTMLæŠ¥å‘Šå†…å®¹"""
        master_summary = report_data['master_summary']
        test_results = report_data['test_category_results']
        recommendations = report_data['system_recommendations']
        
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å°æ™ºç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; color: #333; border-bottom: 2px solid #007bff; padding-bottom: 20px; margin-bottom: 30px; }}
        .summary {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin-bottom: 30px; }}
        .summary-card {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 8px; text-align: center; }}
        .summary-card h3 {{ margin: 0; font-size: 2em; }}
        .summary-card p {{ margin: 5px 0 0 0; }}
        .test-results {{ margin-bottom: 30px; }}
        .test-item {{ background-color: #f8f9fa; margin: 10px 0; padding: 15px; border-radius: 6px; border-left: 4px solid #007bff; }}
        .test-item.success {{ border-left-color: #28a745; }}
        .test-item.failure {{ border-left-color: #dc3545; }}
        .test-title {{ font-weight: bold; margin-bottom: 10px; }}
        .test-details {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 10px; font-size: 0.9em; }}
        .recommendations {{ background-color: #fff3cd; border: 1px solid #ffeaa7; border-radius: 6px; padding: 20px; }}
        .recommendations h3 {{ color: #856404; margin-top: 0; }}
        .recommendations ul {{ margin: 10px 0; }}
        .recommendations li {{ margin: 5px 0; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¤– å°æ™ºç³»ç»Ÿæµ‹è¯•æŠ¥å‘Š</h1>
            <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="summary">
            <div class="summary-card">
                <h3>{master_summary['successful_categories']}/{master_summary['total_test_categories']}</h3>
                <p>æµ‹è¯•é€šè¿‡ç‡</p>
            </div>
            <div class="summary-card">
                <h3>{master_summary['overall_success_rate']:.1f}%</h3>
                <p>ç»¼åˆæˆåŠŸç‡</p>
            </div>
            <div class="summary-card">
                <h3>{master_summary['total_execution_time']:.1f}s</h3>
                <p>æ€»æ‰§è¡Œæ—¶é—´</p>
            </div>
        </div>
        
        <div class="test-results">
            <h2>ğŸ“Š æµ‹è¯•ç»“æœè¯¦æƒ…</h2>
        """
        
        for test_name, result in test_results.items():
            success_class = 'success' if result['success'] else 'failure'
            status_icon = 'âœ…' if result['success'] else 'âŒ'
            
            html += f"""
            <div class="test-item {success_class}">
                <div class="test-title">
                    {status_icon} {test_name.upper()} æµ‹è¯•
                </div>
                <div class="test-details">
                    <div><strong>çŠ¶æ€:</strong> {'é€šè¿‡' if result['success'] else 'å¤±è´¥'}</div>
                    <div><strong>è€—æ—¶:</strong> {result['duration']:.1f}ç§’</div>
                    <div><strong>å­æµ‹è¯•æ•°:</strong> {result['sub_tests']}</div>
                    <div><strong>å­æµ‹è¯•æˆåŠŸç‡:</strong> {result['sub_success_rate']:.1f}%</div>
                </div>
            """
            
            if not result['success'] and result.get('error'):
                html += f"<div style='margin-top: 10px; color: #dc3545;'><strong>é”™è¯¯ä¿¡æ¯:</strong> {result['error']}</div>"
            
            html += "</div>"
        
        html += f"""
        </div>
        
        <div class="recommendations">
            <h3>ğŸ’¡ ç³»ç»Ÿå»ºè®®</h3>
            <ul>
        """
        
        for rec in recommendations:
            html += f"<li>{rec}</li>"
        
        html += """
            </ul>
        </div>
        
    </div>
</body>
</html>
        """
        
        return html
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")
        
        # åœæ­¢ç¡¬ä»¶æ¨¡æ‹Ÿå™¨
        if self.simulator_process:
            try:
                self.simulator_process.terminate()
                await asyncio.sleep(2)
                if self.simulator_process.poll() is None:
                    self.simulator_process.kill()
                logger.info("âœ… ç¡¬ä»¶æ¨¡æ‹Ÿå™¨å·²åœæ­¢")
            except Exception as e:
                logger.error(f"âŒ åœæ­¢ç¡¬ä»¶æ¨¡æ‹Ÿå™¨å¼‚å¸¸: {e}")

def print_final_summary(report: Dict):
    """æ‰“å°æœ€ç»ˆæµ‹è¯•æ€»ç»“"""
    print("\n" + "="*80)
    print("ğŸ¯ å°æ™ºç³»ç»Ÿå…¨å¥—æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    summary = report['master_summary']
    
    print(f"ğŸ“Š æ€»ä½“ç»“æœ:")
    print(f"  æµ‹è¯•ç±»åˆ«æ•°: {summary['total_test_categories']}")
    print(f"  æˆåŠŸç±»åˆ«æ•°: {summary['successful_categories']}")
    print(f"  å¤±è´¥ç±»åˆ«æ•°: {summary['failed_categories']}")
    print(f"  ç»¼åˆæˆåŠŸç‡: {summary['overall_success_rate']:.1f}%")
    print(f"  æ€»æ‰§è¡Œæ—¶é—´: {summary['total_execution_time']:.1f}ç§’")
    
    print(f"\nğŸ“‹ å„æµ‹è¯•ç±»åˆ«:")
    for test_name, result in report['test_category_results'].items():
        status = "âœ… é€šè¿‡" if result['success'] else "âŒ å¤±è´¥"
        print(f"  {test_name.upper()}: {status} (è€—æ—¶: {result['duration']:.1f}s, æˆåŠŸç‡: {result['sub_success_rate']:.1f}%)")
    
    print(f"\nğŸ’¡ ç³»ç»Ÿå»ºè®®:")
    for i, rec in enumerate(report['system_recommendations'], 1):
        print(f"  {i}. {rec}")
    
    # åˆ¤æ–­æ•´ä½“æ˜¯å¦æˆåŠŸ
    overall_success = summary['overall_success_rate'] >= 80
    
    if overall_success:
        print(f"\nğŸ‰ æ­å–œï¼å°æ™ºç³»ç»Ÿæµ‹è¯•æ•´ä½“é€šè¿‡ï¼")
        print(f"   ç³»ç»Ÿè¿è¡ŒçŠ¶å†µè‰¯å¥½ï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨ã€‚")
    else:
        print(f"\nâš ï¸  å°æ™ºç³»ç»Ÿå­˜åœ¨ä¸€äº›é—®é¢˜éœ€è¦è§£å†³ã€‚")
        print(f"   è¯·æ ¹æ®ä¸Šè¿°å»ºè®®è¿›è¡Œæ’æŸ¥å’Œä¿®å¤ã€‚")
    
    print("="*80)

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å°æ™ºç³»ç»Ÿå…¨å¥—æµ‹è¯•ä¸€é”®è¿è¡Œ')
    parser.add_argument('--java-url', default='http://q83b6ed9.natappfree.cc', help='Java APIæœåŠ¡URL')
    parser.add_argument('--python-url', default='http://47.98.51.180:8003', help='Python APIæœåŠ¡URL')
    parser.add_argument('--mqtt-host', default='47.97.185.142', help='MQTTæœåŠ¡å™¨åœ°å€')
    parser.add_argument('--mqtt-port', type=int, default=1883, help='MQTTæœåŠ¡å™¨ç«¯å£')
    parser.add_argument('--websocket-url', default='ws://47.98.51.180:8000/xiaozhi/v1/', help='WebSocketæœåŠ¡å™¨URL')
    parser.add_argument('--device-id', default='f0:9e:9e:04:8a:44', help='æµ‹è¯•è®¾å¤‡ID')
    parser.add_argument('--no-simulator', action='store_true', help='ä¸å¯åŠ¨ç¡¬ä»¶æ¨¡æ‹Ÿå™¨')
    parser.add_argument('--no-stress', action='store_true', help='ä¸è¿è¡Œå‹åŠ›æµ‹è¯•')
    parser.add_argument('--concurrent', type=int, default=3, help='å¹¶å‘æµ‹è¯•æ•°é‡')
    parser.add_argument('--timeout', type=int, default=300, help='å•ä¸ªæµ‹è¯•è¶…æ—¶æ—¶é—´(ç§’)')
    parser.add_argument('--output-dir', default='test_reports', help='æµ‹è¯•æŠ¥å‘Šè¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = TestSuiteConfig()
    config.java_api_url = args.java_url
    config.python_api_url = args.python_url
    config.mqtt_host = args.mqtt_host
    config.mqtt_port = args.mqtt_port
    config.websocket_url = args.websocket_url
    config.test_device_id = args.device_id
    config.enable_hardware_simulator = not args.no_simulator
    config.enable_stress_tests = not args.no_stress
    config.concurrent_tests = args.concurrent
    config.test_timeout = args.timeout
    config.output_dir = args.output_dir
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = TestRunner(config)
    
    try:
        print("ğŸš€ å°æ™ºç³»ç»Ÿå…¨å¥—æµ‹è¯•å¯åŠ¨")
        print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
        print(f"  Java API: {config.java_api_url}")
        print(f"  Python API: {config.python_api_url}")
        print(f"  MQTT: {config.mqtt_host}:{config.mqtt_port}")
        print(f"  WebSocket: {config.websocket_url}")
        print(f"  æµ‹è¯•è®¾å¤‡: {config.test_device_id}")
        print(f"  ç¡¬ä»¶æ¨¡æ‹Ÿå™¨: {'å¯ç”¨' if config.enable_hardware_simulator else 'ç¦ç”¨'}")
        print(f"  å¹¶å‘æµ‹è¯•æ•°: {config.concurrent_tests}")
        print(f"  æµ‹è¯•è¶…æ—¶: {config.test_timeout}ç§’")
        print(f"  è¾“å‡ºç›®å½•: {config.output_dir}")
        print("-" * 60)
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        comprehensive_report = await runner.run_all_tests()
        
        if comprehensive_report.get("success", True):
            # æ‰“å°æœ€ç»ˆæ€»ç»“
            print_final_summary(comprehensive_report)
            
            # è¿”å›æˆåŠŸçŠ¶æ€
            overall_success_rate = comprehensive_report.get('master_summary', {}).get('overall_success_rate', 0)
            return overall_success_rate >= 80
        else:
            print(f"\nâŒ æµ‹è¯•å¥—ä»¶æ‰§è¡Œå¤±è´¥: {comprehensive_report.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return False
        
    except KeyboardInterrupt:
        print("\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        return False
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¥—ä»¶æ‰§è¡Œå¼‚å¸¸: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ¤– å°æ™ºç³»ç»Ÿå…¨å¥—æµ‹è¯•å¥—ä»¶ v1.0.0")
    print("=" * 80)
    
    success = asyncio.run(main())
    
    if success:
        print("\nğŸ‰ æµ‹è¯•å¥—ä»¶æ‰§è¡ŒæˆåŠŸï¼")
        sys.exit(0)
    else:
        print("\nâŒ æµ‹è¯•å¥—ä»¶æ‰§è¡Œå¤±è´¥ï¼")
        sys.exit(1)
