#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤LLMå¹¶å‘è°ƒç”¨é—®é¢˜
è§£å†³rapidæ¨¡å¼ä¸‹OpenAI MissingParameteré”™è¯¯
"""

import asyncio
import time
import threading
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger('LLMå¹¶å‘ä¿®å¤')

class LLMConcurrencyManager:
    """LLMå¹¶å‘è°ƒç”¨ç®¡ç†å™¨"""
    
    def __init__(self, max_concurrent_calls=2, call_interval=1.0):
        self.max_concurrent_calls = max_concurrent_calls  # æœ€å¤§å¹¶å‘è°ƒç”¨æ•°
        self.call_interval = call_interval  # è°ƒç”¨é—´éš”(ç§’)
        
        # å¹¶å‘æ§åˆ¶
        self.semaphore = asyncio.Semaphore(max_concurrent_calls)
        self.last_call_time = 0
        self.call_lock = threading.Lock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_calls = 0
        self.concurrent_calls = 0
        self.error_calls = 0
        
        logger.info(f"ğŸ”§ LLMå¹¶å‘ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        logger.info(f"   æœ€å¤§å¹¶å‘æ•°: {max_concurrent_calls}")
        logger.info(f"   è°ƒç”¨é—´éš”: {call_interval}ç§’")
    
    async def safe_llm_call(self, llm_instance, messages: List[Dict], **kwargs) -> str:
        """å®‰å…¨çš„LLMè°ƒç”¨ï¼Œå¸¦å¹¶å‘æ§åˆ¶"""
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            try:
                self.concurrent_calls += 1
                self.total_calls += 1
                
                # æ§åˆ¶è°ƒç”¨é¢‘ç‡
                await self._rate_limit()
                
                logger.debug(f"ğŸ”„ LLMè°ƒç”¨å¼€å§‹ (å¹¶å‘æ•°: {self.concurrent_calls}/{self.max_concurrent_calls})")
                
                # æ‰§è¡ŒLLMè°ƒç”¨
                response = await asyncio.to_thread(llm_instance.chat, messages, **kwargs)
                
                if response and len(response.strip()) > 0:
                    # æ£€æµ‹é”™è¯¯å“åº”
                    if "OpenAIæœåŠ¡å“åº”å¼‚å¸¸" in response or "MissingParameter" in response:
                        logger.error(f"ğŸš¨ LLMè¿”å›é”™è¯¯: {response[:100]}...")
                        self.error_calls += 1
                        return self._get_fallback_response()
                    
                    logger.debug(f"âœ… LLMè°ƒç”¨æˆåŠŸ: {response[:50]}...")
                    return response.strip()
                else:
                    logger.warning("âš ï¸ LLMè¿”å›ç©ºå“åº”")
                    return self._get_fallback_response()
                    
            except Exception as e:
                self.error_calls += 1
                logger.error(f"âŒ LLMè°ƒç”¨å¼‚å¸¸: {e}")
                
                # åˆ†æé”™è¯¯ç±»å‹
                error_msg = str(e)
                if "MissingParameter" in error_msg:
                    logger.error("ğŸ” æ£€æµ‹åˆ°MissingParameter - å¯èƒ½æ˜¯å¹¶å‘è°ƒç”¨å¯¼è‡´çš„å‚æ•°é—®é¢˜")
                elif "timeout" in error_msg.lower():
                    logger.error("ğŸ” æ£€æµ‹åˆ°è¶…æ—¶ - å¯èƒ½æ˜¯å¹¶å‘è°ƒç”¨è¿‡å¤š")
                elif "rate" in error_msg.lower() or "limit" in error_msg.lower():
                    logger.error("ğŸ” æ£€æµ‹åˆ°é™æµ - APIè°ƒç”¨è¿‡äºé¢‘ç¹")
                
                return self._get_fallback_response()
                
            finally:
                self.concurrent_calls -= 1
    
    async def _rate_limit(self):
        """è°ƒç”¨é¢‘ç‡é™åˆ¶"""
        with self.call_lock:
            current_time = time.time()
            time_since_last_call = current_time - self.last_call_time
            
            if time_since_last_call < self.call_interval:
                sleep_time = self.call_interval - time_since_last_call
                logger.debug(f"â³ é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {sleep_time:.2f} ç§’")
                
        # åœ¨é”å¤–è¿›è¡Œsleep
        if 'sleep_time' in locals():
            await asyncio.sleep(sleep_time)
        
        with self.call_lock:
            self.last_call_time = time.time()
    
    def _get_fallback_response(self) -> str:
        """è·å–å¤‡ç”¨å“åº”"""
        fallback_responses = [
            "æ”¶åˆ°æ¶ˆæ¯ï¼Œè¯·æ³¨æ„æŸ¥çœ‹ã€‚",
            "æ¶ˆæ¯æé†’ï¼Œè¯·åŠæ—¶å…³æ³¨ã€‚",
            "ä¿¡æ¯æ›´æ–°ï¼Œè¯·æŸ¥çœ‹è¯¦æƒ…ã€‚"
        ]
        import random
        return random.choice(fallback_responses)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_calls": self.total_calls,
            "concurrent_calls": self.concurrent_calls,
            "error_calls": self.error_calls,
            "error_rate": f"{(self.error_calls/max(self.total_calls,1)*100):.1f}%",
            "max_concurrent": self.max_concurrent_calls,
            "call_interval": f"{self.call_interval}ç§’"
        }

# å…¨å±€å¹¶å‘ç®¡ç†å™¨
_concurrency_manager: Optional[LLMConcurrencyManager] = None

def get_llm_concurrency_manager() -> LLMConcurrencyManager:
    """è·å–å…¨å±€LLMå¹¶å‘ç®¡ç†å™¨"""
    global _concurrency_manager
    if _concurrency_manager is None:
        _concurrency_manager = LLMConcurrencyManager(
            max_concurrent_calls=2,  # é™åˆ¶æœ€å¤§å¹¶å‘ä¸º2
            call_interval=0.8        # æ¯æ¬¡è°ƒç”¨é—´éš”0.8ç§’
        )
    return _concurrency_manager

def patch_unified_event_service_for_concurrency():
    """ä¸ºUnifiedEventServiceæ‰“å¹¶å‘æ§åˆ¶è¡¥ä¸"""
    try:
        import sys
        from core.services.unified_event_service import UnifiedEventService
        
        # å¤‡ä»½åŸå§‹çš„LLMè°ƒç”¨æ–¹æ³•
        original_generate = UnifiedEventService._generate_content_with_java_prompt
        
        async def safe_generate_with_concurrency(self, event_data: Dict[str, Any]) -> Optional[str]:
            """å¸¦å¹¶å‘æ§åˆ¶çš„å†…å®¹ç”Ÿæˆ"""
            try:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦LLMå¤„ç†
                prompt = event_data.get("prompt")
                result = (event_data.get("result") or 
                         event_data.get("content") or 
                         event_data.get("data") or 
                         event_data.get("festival"))
                
                if not prompt or not result or not self.llm:
                    return await original_generate(self, event_data)
                
                # ä½¿ç”¨å¹¶å‘ç®¡ç†å™¨è¿›è¡ŒLLMè°ƒç”¨
                manager = get_llm_concurrency_manager()
                
                # æ„å»ºmessages
                messages = [
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": f"è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆå›å¤ï¼š{result}"}
                ]
                
                # å®‰å…¨çš„LLMè°ƒç”¨
                response = await manager.safe_llm_call(self.llm, messages)
                
                logger.info(f"ğŸ¯ LLMå¹¶å‘è°ƒç”¨å®Œæˆ: {response[:50]}...")
                return response
                
            except Exception as e:
                logger.error(f"å¹¶å‘LLMè°ƒç”¨å¤±è´¥: {e}")
                # é™çº§åˆ°åŸå§‹æ–¹æ³•
                return await original_generate(self, event_data)
        
        # æ›¿æ¢æ–¹æ³•
        UnifiedEventService._generate_content_with_java_prompt = safe_generate_with_concurrency
        logger.info("âœ… UnifiedEventServiceå¹¶å‘æ§åˆ¶è¡¥ä¸å·²åº”ç”¨")
        
    except Exception as e:
        logger.error(f"âŒ å¹¶å‘æ§åˆ¶è¡¥ä¸åº”ç”¨å¤±è´¥: {e}")

async def test_concurrent_llm_calls():
    """æµ‹è¯•å¹¶å‘LLMè°ƒç”¨"""
    logger.info("ğŸ§ª æµ‹è¯•LLMå¹¶å‘è°ƒç”¨æ§åˆ¶")
    logger.info("="*40)
    
    class MockLLM:
        def __init__(self):
            self.call_count = 0
        
        def chat(self, messages, **kwargs):
            self.call_count += 1
            call_id = self.call_count
            
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            time.sleep(0.5)
            
            # æ¨¡æ‹Ÿå¶å‘çš„å¹¶å‘é—®é¢˜
            if call_id > 1 and call_id % 3 == 0:
                raise Exception("OpenAIæœåŠ¡å“åº”å¼‚å¸¸: MissingParameter")
            
            return f"å“åº”å†…å®¹ {call_id} - å¤„ç†æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}"
    
    mock_llm = MockLLM()
    manager = LLMConcurrencyManager(max_concurrent_calls=2, call_interval=0.5)
    
    # æ¨¡æ‹Ÿå¿«é€Ÿå‘é€5æ¡æ¶ˆæ¯ï¼ˆç±»ä¼¼rapidæµ‹è¯•ï¼‰
    test_messages = [
        [{"role": "user", "content": f"æµ‹è¯•æ¶ˆæ¯ {i}"}] 
        for i in range(1, 6)
    ]
    
    logger.info(f"ğŸ“¤ æ¨¡æ‹Ÿå‘é€ {len(test_messages)} æ¡å¹¶å‘æ¶ˆæ¯")
    
    # å¹¶å‘å‘é€æ‰€æœ‰æ¶ˆæ¯
    start_time = time.time()
    tasks = [
        manager.safe_llm_call(mock_llm, messages) 
        for messages in test_messages
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    end_time = time.time()
    
    # æ˜¾ç¤ºç»“æœ
    logger.info(f"ğŸ“Š å¹¶å‘æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    
    for i, result in enumerate(results, 1):
        if isinstance(result, Exception):
            logger.error(f"   æ¶ˆæ¯ {i}: âŒ {result}")
        else:
            logger.info(f"   æ¶ˆæ¯ {i}: âœ… {result}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_stats()
    logger.info("ğŸ“ˆ è°ƒç”¨ç»Ÿè®¡:")
    for key, value in stats.items():
        logger.info(f"   {key}: {value}")

def create_improved_test_script():
    """åˆ›å»ºæ”¹è¿›çš„æµ‹è¯•è„šæœ¬"""
    improved_script = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„æ¶ˆæ¯é˜Ÿåˆ—æµ‹è¯• - æ”¯æŒLLMå¹¶å‘æ§åˆ¶
"""

import asyncio
import time
from datetime import datetime
import logging

# åº”ç”¨å¹¶å‘æ§åˆ¶è¡¥ä¸
from ä¿®å¤LLMå¹¶å‘é—®é¢˜ import patch_unified_event_service_for_concurrency
patch_unified_event_service_for_concurrency()

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger('æ”¹è¿›é˜Ÿåˆ—æµ‹è¯•')

DEVICE_ID = "f0:9e:9e:04:8a:44"

async def test_rapid_with_concurrency_control():
    """å¸¦å¹¶å‘æ§åˆ¶çš„å¿«é€Ÿæ¶ˆæ¯æµ‹è¯•"""
    logger.info("ğŸ§ª æµ‹è¯•å¸¦å¹¶å‘æ§åˆ¶çš„å¿«é€Ÿæ¶ˆæ¯å‘é€")
    logger.info("="*50)
    
    messages = [
        {"title": "å¤©æ°”é¢„è­¦", "prompt": "å¤§é£é¢„è­¦ï¼Œè¯·æ³¨æ„å®‰å…¨", "priority": 0},
        {"title": "èŠ‚æ—¥é—®å€™", "prompt": "èŠ‚æ—¥å¿«ä¹ï¼Œç¥æ‚¨æ„‰å¿«", "priority": 1},  
        {"title": "24èŠ‚æ°”", "prompt": "ç«‹ç§‹æ—¶èŠ‚ï¼Œæ³¨æ„å…»ç”Ÿ", "priority": 1},
        {"title": "å¤©æ°”æ’­æŠ¥", "prompt": "ä»Šæ—¥æ™´æœ—ï¼Œæ¸©åº¦é€‚å®œ", "priority": 2},
    ]
    
    logger.info(f"ğŸ“‹ å‡†å¤‡å‘é€ {len(messages)} æ¡æ¶ˆæ¯ï¼ˆé—´éš”1ç§’ï¼Œé¿å…å¹¶å‘å†²çªï¼‰")
    
    for i, msg in enumerate(messages, 1):
        logger.info(f"ğŸ“¤ [{i}/{len(messages)}] å‘é€: {msg['title']}")
        
        # å‘é€æ¶ˆæ¯çš„APIè°ƒç”¨...
        await asyncio.sleep(1.0)  # å¢åŠ é—´éš”åˆ°1ç§’ï¼Œé¿å…å¹¶å‘é—®é¢˜
        
        logger.info(f"   âœ… æ¶ˆæ¯ {i} å‘é€å®Œæˆ")
    
    logger.info("âœ… æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆï¼ŒLLMå¹¶å‘å¾—åˆ°æœ‰æ•ˆæ§åˆ¶")

if __name__ == "__main__":
    asyncio.run(test_rapid_with_concurrency_control())
'''
    
    with open("æ”¹è¿›çš„æ¶ˆæ¯é˜Ÿåˆ—æµ‹è¯•.py", "w", encoding="utf-8") as f:
        f.write(improved_script)
    
    logger.info("ğŸ“„ åˆ›å»ºäº†æ”¹è¿›çš„æµ‹è¯•è„šæœ¬: æ”¹è¿›çš„æ¶ˆæ¯é˜Ÿåˆ—æµ‹è¯•.py")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ”§ LLMå¹¶å‘è°ƒç”¨é—®é¢˜ä¿®å¤å·¥å…·")
    logger.info("="*40)
    
    print("ğŸ¯ ä¿®å¤æ–¹æ¡ˆ:")
    print("1. åº”ç”¨å¹¶å‘æ§åˆ¶è¡¥ä¸")
    print("2. æµ‹è¯•å¹¶å‘LLMè°ƒç”¨")
    print("3. åˆ›å»ºæ”¹è¿›çš„æµ‹è¯•è„šæœ¬")
    print()
    
    choice = input("é€‰æ‹©æ“ä½œ (1-3, æˆ–æŒ‰å›è½¦å…¨éƒ¨æ‰§è¡Œ): ").strip()
    
    if choice == "1" or not choice:
        patch_unified_event_service_for_concurrency()
    
    if choice == "2" or not choice:
        asyncio.run(test_concurrent_llm_calls())
    
    if choice == "3" or not choice:
        create_improved_test_script()
    
    print()
    print("ğŸ‰ ä¿®å¤å®Œæˆï¼å»ºè®®:")
    print("1. é‡å¯PythonæœåŠ¡ä»¥ä½¿è¡¥ä¸ç”Ÿæ•ˆ")
    print("2. ä½¿ç”¨ python æ”¹è¿›çš„æ¶ˆæ¯é˜Ÿåˆ—æµ‹è¯•.py æµ‹è¯•")
    print("3. æˆ–è€…åœ¨rapidæµ‹è¯•ä¸­å¢åŠ æ¶ˆæ¯é—´éš”åˆ°1-2ç§’")

if __name__ == "__main__":
    main()
